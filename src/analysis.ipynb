{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree to png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treelib import Tree, Node\n",
    "import pydot as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(tag=ch2, identifier=ch2, data=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tree()\n",
    "t.create_node('root','root')\n",
    "t.create_node('ch1','ch1',parent='root')\n",
    "t.create_node('ch2','ch2',parent='root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.to_graphviz('test.gv')\n",
    "dot = pd.graph_from_dot_file('test.gv')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot.write_png('test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing score and heuristic\n",
    "\n",
    "starting with evaluating the performance of regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from time import perf_counter_ns\n",
    "from model.grid import Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average = 0.005135909966863656 ms\n",
      "Max = 35.9908 ms\n"
     ]
    }
   ],
   "source": [
    "reg = re.compile(\"(1(?!1{3,})|0(?!0{3,})){4,}\")\n",
    "\n",
    "avg = 0\n",
    "max = 0\n",
    "for i in range(4000000):\n",
    "    start = perf_counter_ns()\n",
    "    j = 0\n",
    "    matches = reg.finditer(\"00001201010\")\n",
    "    for match in matches:\n",
    "        j += 1\n",
    "    d = (perf_counter_ns() - start) / 1e6\n",
    "    avg = (avg +  d)/ 2\n",
    "    if max < d: max = d\n",
    "print(\"Average =\",avg,\"ms\")\n",
    "print(\"Max =\",max,\"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of regex is in the range of microseconds. It is not the problem, these unrealistic numbers are coming from somewhere else in the score and heuristic functions\n",
    "\n",
    "## using regex for scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.23693513147081263 ms.\n",
      "max = 3.5371 ms\n"
     ]
    }
   ],
   "source": [
    "g = Grid()\n",
    "avg = 0\n",
    "max = 0\n",
    "for i in range(1000):\n",
    "    start = perf_counter_ns()\n",
    "    score = g.get_score()\n",
    "    duration = (perf_counter_ns() - start) / 1e6\n",
    "    avg  = (avg + duration) / 2\n",
    "    if duration > max: max = duration\n",
    "print(\"avg =\",avg,\"ms.\")\n",
    "print(\"max =\",max,\"ms\")\n",
    "# how much it would take to evaluate for each of these depth roughly\n",
    "time = avg\n",
    "k = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for i in k : print(round(time * 7 ** i,3) , \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minor optimization: which representation of the elements is more efficient at conversion to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016249998956445748\n",
      "0.0003000015735742899\n"
     ]
    }
   ],
   "source": [
    "avgchr = 0\n",
    "avgint = 0\n",
    "for i in range(10000000):\n",
    "    t = perf_counter_ns()\n",
    "    chr(65)\n",
    "    avgchr = ((avgchr + (perf_counter_ns() - t) / 1e6)) / 2\n",
    "    t = perf_counter_ns()\n",
    "    str(1)\n",
    "    avgint = (avgint + ((perf_counter_ns() - t) / 1e6)) /2\n",
    "print(avgchr)\n",
    "print(avgint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using bytes is better than ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.20091451739831348 ms.\n",
      "max = 1.103 ms\n"
     ]
    }
   ],
   "source": [
    "from model.grid import Grid\n",
    "from time import perf_counter_ns\n",
    "\n",
    "g = Grid()\n",
    "avg = 0\n",
    "max = 0\n",
    "for i in range(1000):\n",
    "    start = perf_counter_ns()\n",
    "    score = g.get_score()\n",
    "    duration = (perf_counter_ns() - start) / 1e6\n",
    "    avg  = (avg + duration) / 2\n",
    "    if duration > max: max = duration\n",
    "print(\"avg =\",avg,\"ms.\")\n",
    "print(\"max =\",max,\"ms\")\n",
    "# how much it would take to evaluate for each of these depth roughly\n",
    "time = avg\n",
    "k = [i+1 for i in range(13)]\n",
    "for i in k : print(round(time * 7 ** i,3) , \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replacing regex with manual string search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.14025233685007482 ms.\n",
      "max = 2.1745 ms\n"
     ]
    }
   ],
   "source": [
    "from model.grid import Grid\n",
    "from time import perf_counter_ns\n",
    "\n",
    "g = Grid()\n",
    "avg = 0\n",
    "max = 0\n",
    "for i in range(1000):\n",
    "    start = perf_counter_ns()\n",
    "    score = g.get_score()\n",
    "    duration = (perf_counter_ns() - start) / 1e6\n",
    "    avg  = (avg + duration) / 2\n",
    "    if duration > max: max = duration\n",
    "print(\"avg =\",avg,\"ms.\")\n",
    "print(\"max =\",max,\"ms\")\n",
    "# how much it would take to evaluate for each of these depth roughly\n",
    "time = avg\n",
    "k = [i+1 for i in range(13)]\n",
    "for i in k : print(round(time * 7 ** i,3) , \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 0.2602384100792934 ms.\n",
      "max = 3.6305 ms\n",
      "1.822 seconds.\n",
      "12.752 seconds.\n",
      "89.262 seconds.\n",
      "624.832 seconds.\n",
      "4373.827 seconds.\n",
      "30616.789 seconds.\n",
      "214317.521 seconds.\n",
      "1500222.647 seconds.\n",
      "10501558.527 seconds.\n",
      "73510909.687 seconds.\n",
      "514576367.806 seconds.\n",
      "3602034574.639 seconds.\n",
      "25214242022.474 seconds.\n"
     ]
    }
   ],
   "source": [
    "from model.grid import Grid\n",
    "from time import perf_counter_ns\n",
    "\n",
    "g = Grid()\n",
    "avg = 0\n",
    "max = 0\n",
    "for i in range(1000):\n",
    "    start = perf_counter_ns()\n",
    "    score = g.get_heuristic_value()\n",
    "    duration = (perf_counter_ns() - start) / 1e6\n",
    "    avg  = (avg + duration) / 2\n",
    "    if duration > max: max = duration\n",
    "print(\"avg =\",avg,\"ms.\")\n",
    "print(\"max =\",max,\"ms\")\n",
    "# how much it would take to evaluate for each of these depth roughly\n",
    "time = avg\n",
    "k = [i+1 for i in range(13)]\n",
    "for i in k : print(round(time * 7 ** i,3) , \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change heuristic to manual string search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = 1.9824533947657281 ms.\n",
      "max = 9.1057 ms\n",
      "13.877 seconds.\n",
      "97.14 seconds.\n",
      "679.982 seconds.\n",
      "4759.871 seconds.\n",
      "33319.094 seconds.\n",
      "233233.659 seconds.\n",
      "1632635.616 seconds.\n",
      "11428449.313 seconds.\n",
      "79999145.188 seconds.\n",
      "559994016.317 seconds.\n",
      "3919958114.221 seconds.\n",
      "27439706799.55 seconds.\n",
      "192077947596.849 seconds.\n"
     ]
    }
   ],
   "source": [
    "from model.grid import Grid\n",
    "from time import perf_counter_ns\n",
    "\n",
    "g = Grid()\n",
    "avg = 0\n",
    "max = 0\n",
    "for i in range(1000):\n",
    "    start = perf_counter_ns()\n",
    "    score = g.get_heuristic_value()\n",
    "    duration = (perf_counter_ns() - start) / 1e6\n",
    "    avg  = (avg + duration) / 2\n",
    "    if duration > max: max = duration\n",
    "print(\"avg =\",avg,\"ms.\")\n",
    "print(\"max =\",max,\"ms\")\n",
    "# how much it would take to evaluate for each of these depth roughly\n",
    "time = avg\n",
    "k = [i+1 for i in range(13)]\n",
    "for i in k : print(round(time * 7 ** i,3) , \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes\n",
    "\n",
    "- Heuristic will remain with regex\n",
    "- Score will use manual string search\n",
    "- Grid will be represented as bytes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f8a7b48fb9feb5c77e6c1a2df3c1395a643bd8f65c81128a8ee331708e7519c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
